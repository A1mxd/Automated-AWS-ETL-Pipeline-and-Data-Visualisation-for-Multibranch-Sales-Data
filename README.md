# `COOL BEANS FINAL PROJECT `

### We are building a system to create statistics and analysis of data of what client is selling and how to grow customers by keeping existing ones and reaching new customers.

----

# Description

The café has seen unprecedented growth and has expanded to hundreds of outlets across the country. Due to the demand the company is receiving, they need to figure out how they can best target new and returning customers, and also understand which products are selling well. Our client are experiencing issues with collating and analysing the data they are producing at each branch, as their technical setup is limited.

This pipeline will collect all the transaction data generated by each individual café and place it in a single location. By being able to easily query the company's data as a whole, the client will drastically increase their ability to identify company-wide trends and insights.

----

# Elevator Pitch

**For** the existing client 
**who** needs an updated system to figure out how they can best target new and returning customers, and also understand which products are selling well, 
**The Cool Beans System** is an identifying, tracking and analysing system of trends of the performance of their business. 
**Unlike** their previous system, 
**our product** will obtain, process, store, collate and analyse the data they are producing at each branch and increase their ability to identify company-wide trends and insights.
----

----
# The MoSCoW

### Must
* Data visualisation of branches  
* Data storage in CSV
* Importing and exporting function
* ETL
* Collect sales data from each branch  
* Application monitoring software will be used to produce operational metrics, such as system errors, up-time and more
* GUI
* Error checking / Unit test

### Should
* Frequency of purchase of items
* General customer data 
* User friendly app

### Could
* Tracking what products are purchased together
* Personalised customer data 
* Data about specific locations

### Won't
* Handling Courier data 
* Compare products

----
# The Approach

![cool_beans_final_project_approach](https://user-images.githubusercontent.com/127961112/232329510-b22ddc61-f802-4859-bfb2-4ccae084ba5a.png)

----

# Getting Started

## Dependencies

* As AWS Lambdas only being able to support Python up to 3.10, we use `Python 3.10` for this project. 
* We have started working with `Docker` and `PostgreSQL` for database, now we use `AWS Redshift`.
* We work with Generation UK student AWS account and `eu-west-1` is our AWS Region. The AWS services we use in this project are: `S3 bucket`, `Lambda`, `Cloudformation`, `Cloudwatch`, `Redshift`.
* We monitor infrastructure of our application with `Cloudwatch` and `Grafana`.
* We work remotely on `GitHub` group repository as a team.

## Working directory

We have; `src` folder for our main app, `test` for test modules, `data` for the CSV files, and `.venv` folder under the main `cool-beans-final-project` directory. On this main directory, we keep this `README.md`, `docker-compose.yml`, `.env`, `run-stack.sh`, `cool-beans-aws-lambda-template.yml`, `requirements.txt`, and `.gitignore`.

## Installing

Required libraries and modules for our main app:
```
import os
import csv
from datetime import datetime
import psycopg2
from dotenv import load_dotenv
```
Required libraries and modules for our test modules:
```
from unittest.mock import patch
import unittest
import sys
```

## Command-Line Interface for installations and running of the application 

We use following commands;

* to run a module from main dir (`cool-beans-final-project directory`) : `py -m src.create_database`

FOR PostgreSQL and Docker container:
* to install psycopg2: `py -m pip install psycopg2` 
* to install python-dotenv: `py -m pip install python-dotenv`
* to pull PostgreSQL docker image: `docker pull postgres`
* to start the container: `docker-compose up -d`  

FOR unittesting:
* to install pytest: `python3 -m pip install pytest`
* to run single test: `py -m pytest test/test_convert_all_dates.py -vv`
* to run all the tests: `py -m pytest -vv`

FOR AWS:
* to login AWS: `aws sso login --profile ${profile_name}`

FOR AWS Cloudformation:
* to create a bucket:  `aws s3 mb s3://cool-beans-s3-bucket-stack-1 --profile ${profile_name} --region eu-west-1  `   
* to put a zip file into a bucket: `aws s3 cp cool-beans-csv-reader-lambda-function.zip s3://cool-beans-s3-bucket-stack-1 --profile ${profile_name} --region eu-west-1 `
* to create a stack, we make a file called `run-stack.sh` with this command in it: `aws cloudformation deploy --stack-name cool-beans-final-project --template-file cool-beans-aws-lambda-template.yml --region eu-west-1 --capabilities CAPABILITY_IAM --profile ${profile_name}`
then make it executable by running: `chmod a+x run-stack.sh` and finally we run the `run-stack.sh` file with `./run-stack.sh ${profile_name}` command on bash terminal from the main dir.

FOR Grafana:
* to set up a docket container for Grafana: `docker run -d -p 3000:3000 grafana/grafana`
* to pull Grafana docker image: `docker pull grafana/grafana`

----


### Executing program

* How to run the program
* Step-by-step bullets
```
code blocks for commands
```

----
## ETL process

* We use dummy data in CSV files created by instructors for different branches of the coffee shop. Data includes: 
```date-time, location, customer_name, basket_items, total_price, payment_method, card_number``` 
* We extract data from each CSV file and put each transaction as a dictionary in a list: `transaction_list`
* We get rid of sensitive data: ```customer_name``` and ```card_number```

*** TBC

----

## Help

Any advise for common problems or issues.
```
command to run if program contains helper info
```

## Authors

Contributors names and contact info

* [Joshua Adelowo](https://github.com/joshuaadel0w0)
* [Ahmed Afrah](https://github.com/A1mxd)
* [Muslime Ersoy (Lili)](https://github.com/lili-me)
* [Rosalind Robert](https://github.com/RosalindRobert)

## Version History

* 0.2
    * 

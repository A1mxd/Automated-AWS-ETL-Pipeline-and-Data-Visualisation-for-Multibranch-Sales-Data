# `COOL BEANS FINAL PROJECT `

### We are building a system to create statistics and analysis of data of what client is selling and how to grow customers by keeping existing ones and reaching new customers.

----

# Description

The café has seen unprecedented growth and has expanded to hundreds of outlets across the country. Due to the demand the company is receiving, they need to figure out how they can best target new and returning customers, and also understand which products are selling well. Our client are experiencing issues with collating and analysing the data they are producing at each branch, as their technical setup is limited.

This pipeline will collect all the transaction data generated by each individual café and place it in a single location. By being able to easily query the company's data as a whole, the client will drastically increase their ability to identify company-wide trends and insights.

----

# Elevator Pitch

**For** the existing client 
**who** needs an updated system to figure out how they can best target new and returning customers, and also understand which products are selling well, 
**The Cool Beans System** is an identifying, tracking and analysing system of trends of the performance of their business. 
**Unlike** their previous system, 
**our product** will obtain, process, store, collate and analyse the data they are producing at each branch and increase their ability to identify company-wide trends and insights.
----


----
# The Approach

![cool_beans_final_project_approach](https://user-images.githubusercontent.com/127961112/232329510-b22ddc61-f802-4859-bfb2-4ccae084ba5a.png)

# The Final Structure of Cool Beans Data Engineering Project
![](documentation\cool-beans-data-engineering-project.jpg)

----

# Getting Started

## Dependencies

* As AWS Lambdas only being able to support Python up to 3.10, we use `Python 3.10` for this project. 
* We have started working with `Docker` and `PostgreSQL` for database, now we use `AWS Redshift`.
* We work with Generation UK student AWS account and `eu-west-1` is our AWS Region. The AWS services we use in this project are: `S3 bucket`, `Lambda`, `Cloudformation`, `Cloudwatch`, `Redshift`.
* We monitor infrastructure of our application with `Cloudwatch` and `Grafana`.
* We work remotely on `GitHub` group repository as a team.

## Working directory

We have; `src` folder for our main app, `test` for test modules, `data` for the CSV files, and `.venv` folder under the main `cool-beans-final-project` directory. On this main directory, we keep this `README.md`, `docker-compose.yml`, `.env`, `run-stack.sh`, `cool-beans-aws-lambda-template.yml`, `requirements.txt`, and `.gitignore`.

## Installing

Required libraries and modules for our main app:
```
import os
import csv
from datetime import datetime
import psycopg2
from dotenv import load_dotenv
```
Required libraries and modules for our test modules:
```
from unittest.mock import patch
import unittest
import sys
```
Required libraries and modules for AWS functions are in requirements-lambda.txt

## Command-Line Interface for installations and running of the application 

We use following commands;

* to run a module from main dir (`cool-beans-final-project directory`): `py -m src.create_database`
* to run a module from sub dir (`etl.p`): `py etl.py`

FOR PostgreSQL and Docker container:
* to install psycopg2: `py -m pip install psycopg2` 
* to install python-dotenv: `py -m pip install python-dotenv`
* to pull PostgreSQL docker image: `docker pull postgres`
* to start the container: `docker-compose up -d`  

FOR unittesting:
* to install pytest: `python3 -m pip install pytest`
* to run single test: `py -m pytest test/test_convert_all_dates.py -vv`
* to run all the tests: `py -m pytest -vv`

FOR AWS:
* to login AWS: `aws sso login --profile ${profile_name}`

FOR AWS Cloudformation:
Before CI/CD: 
* to create a bucket:  `aws s3 mb s3://cool-beans-s3-bucket-stack-1 --profile ${profile_name} --region eu-west-1  `   
* to put a zip file into a bucket: `aws s3 cp cool-beans-csv-reader-lambda-function.zip s3://cool-beans-s3-bucket-stack-1 --profile ${profile_name} --region eu-west-1 `
* to create a stack, we make a file called `run-stack.sh` with this command in it: `aws cloudformation deploy --stack-name cool-beans-final-project --template-file cool-beans-aws-lambda-template.yml --region eu-west-1 --capabilities CAPABILITY_IAM --profile ${profile_name}`
then make it executable by running: `chmod a+x run-stack.sh` and finally we run the `run-stack.sh` file with `./run-stack.sh ${profile_name}` command on bash terminal from the main dir.
After CI/CD:
* The AWS Cloudformation is deployed automatically with the GitHub Actions Workflow when we merge to the main.

FOR Grafana:
* to set up a docket container for Grafana: `docker run -d -p 3000:3000 grafana/grafana`
* to pull Grafana docker image: `docker pull grafana/grafana`

FOR AWS EC2 Instance to connect Grafana:

![](documentation\AWS-EC2-Instance-to-connect-Grafana.png)

----

# Cloudwatch metrics visualisation on Grafana examples:
![](documentation\grafana_cloudwatch_metrics\cool-beans-cloudwatch-visualisation-1.jpeg)
![](documentation\grafana_cloudwatch_metrics\cool-beans-cloudwatch-visualisation-2.jpg)

# Redshift data visualisation on Grafana examples:
![](documentation\grafana_redshift_visualisations\cool-beans-redshift-visualisation-1.jpeg)
![](documentation\grafana_redshift_visualisations\cool-beans-redshift-visualisation-2.jpg)
![](documentation\grafana_redshift_visualisations\cool-beans-redshift-visualisation-3.jpg)
![](documentation\grafana_redshift_visualisations\cool-beans-redshift-visualisation-4.jpg)

----

----
## ETL process

* We use dummy data in CSV files created by instructors for different branches of the coffee shop. Data includes: 
```date-time, location, customer_name, basket_items, total_price, payment_method, card_number``` 
* We extract data from each CSV file and put each transaction as a dictionary in a list: `transaction_list`
* We get rid of sensitive data: ```customer_name``` and ```card_number```
* We change format of the date(YYYY-MM-DD hh:mm).
* We split items from the basket in the transactions to get a dictionary of items with prices and id number associated to the transaction.
* We get unique items list of dictionaries.
* We get unique locations list.
* We load locations, items, transaction_items, transactions to the database.

## Lambda Functions
* We have 1 lambda function to create database.
* We have 2 lambda functions for extract-transform and load processes. 
* Those functions linked by SQS and S3 bucket for transformed data. 

## SQS
* We have 2 SQS queues. One for sending message to extract-transform lambda when raw data comes into raw-data-S3 bucket. The other for sending message to load lambda when transformed data comes into transformed-data-s3-bucket.

## S3
* We have 3 S3 buckets. One for raw data, one for transformed data and one for deployment the cloudformation.
----


## Authors

Contributors names and contact info

* [Joshua Adelowo](https://github.com/joshuaadel0w0)
* [Ahmed Afrah](https://github.com/A1mxd)
* [Muslime Ersoy (Lili)](https://github.com/lili-me)
* [Rosalind Robert](https://github.com/RosalindRobert)

## Version History

* 0.2
    * 
